crawler_cnstock.py的注释

WebCrawlFromcnstock类
	几个属性需要输入：
	1.ThreadsNum	线程数
	2.IP	连接数据库的IP
	3.PORT	连接数据库的port
	4.dbName	数据库名称
	5.colName	collection name
 
	使用：	作为外部使用者，我们只需要使用coroutine_run方法即可
		 	但是要确保有Mongo数据库且ID 和 PORT要输入正确
	发现的问题：
			现在总网页上没有手动点页数的功能，而是改成了一个javascript来点击翻页
			所以CrawlHistoryCompanyNews方法中的翻页算法不行了

	内置的方法：
	1.ConnDB(): 连接数据库对应IP,PORT, dbName, colName的数据
				并放入 self._collection 中

	2.countchn(str):字符串分析方法
					返回一个tuple，包含两个数据，(chnnum, possible)
					一个是去除了str的所有非中文字符的字符串
					另一个是 中文字符总数 与 str字符总数 的比值

	3.getUrlInfo(url):返回url里的时间和正文

	4.GenPagesLst(totalPages,Range,initPageID):
				返回一个list，里面包含从initPageID 到 totalPages并按Range 划分的数字
				如(100,10,1)为例，最终得到
				[(1,10), (11,20), (21,30), ..., (91,100)]

	5.extractData(tag_list):
				返回一个list，其中包含打开的数据库中的列名称对应tag_list中的的数据的信息
				tag_list是一个list，其中包含字符串

	6.CrawlHistoryCompanyNews(startPage,endPage,url_Part_1)：
				做两件事：
				1.对 url_Part_1 的startPage-endPage页的具体内容进行爬取

				2.将http://company.cnstock.com/company/scp_gsxw/各个新闻的时间，具体地址，标题和具体正文
				  以data = {'Date' : date,
                            'Address' : a['href'],
                            'Title' : a['title'],
                            'Article' : article}
                  的形式存储到数据库中

--->7.coroutine_run(totalPages,Range,initPageID,**kwarg):
				要爬总网页的多少页（totalPages）且每次爬多少页（Range），从第几页开始爬(initPageID)
				先调用GenPagesLst函数，得到一个包含页数划分的list
				遍历这个list，将每个元素中的第一个元素和第二个元素作为
				startPage 和 endPage 参数传入CrawlHistoryCompanyNews函数中
				而传入coroutine_run的关键字 url_Part_1 也会作为参数传入CrawlHistoryCompanyNews中
				最终利用gevent架构的多线程来爬去多页数据
